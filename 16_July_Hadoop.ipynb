{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91ac8f-c333-43e2-8ac8-a63bfa1b9bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "954f2373-ce8a-4d13-97d4-144e05dcf4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'core-site' section found in the configuration file.\n"
     ]
    }
   ],
   "source": [
    "# 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "\n",
    "import configparser\n",
    "\n",
    "def display_core_components(config_file):\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "    \n",
    "    # Read the Hadoop configuration file\n",
    "    config.read(config_file)\n",
    "    \n",
    "    # Check if the 'core-site' section exists in the configuration file\n",
    "    if 'core-site' in config:\n",
    "        # Get the options in the 'core-site' section\n",
    "        options = config['core-site']\n",
    "        \n",
    "        # Display the core components of Hadoop\n",
    "        if 'fs.defaultFS' in options:\n",
    "            print(\"Filesystem Name: \", options['fs.defaultFS'])\n",
    "        if 'yarn.resourcemanager.address' in options:\n",
    "            print(\"Resource Manager: \", options['yarn.resourcemanager.address'])\n",
    "        if 'mapreduce.framework.name' in options:\n",
    "            print(\"MapReduce Framework: \", options['mapreduce.framework.name'])\n",
    "    else:\n",
    "        print(\"No 'core-site' section found in the configuration file.\")\n",
    "\n",
    "# Provide the path to your Hadoop configuration file\n",
    "config_file_path = '/path/to/hadoop/conf/core-site.xml'\n",
    "\n",
    "# Call the function to display the core components\n",
    "display_core_components(config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4788762-f030-4670-8151-f67114939df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "\n",
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "def calculate_directory_size(hdfs_host, hdfs_port, directory_path):\n",
    "    # Create a PyWebHdfsClient object\n",
    "    hdfs = PyWebHdfsClient(host=hdfs_host, port=hdfs_port)\n",
    "\n",
    "    # Get the status of the directory\n",
    "    directory_status = hdfs.list_dir(directory_path)\n",
    "\n",
    "    # Calculate the total file size\n",
    "    total_size = 0\n",
    "    for file in directory_status['FileStatuses']['FileStatus']:\n",
    "        total_size += file['length']\n",
    "\n",
    "    # Return the total file size\n",
    "    return total_size\n",
    "\n",
    "# Provide the HDFS host, port, and directory path\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9870\n",
    "directory_path = '/path/to/hdfs/directory'\n",
    "\n",
    "# Call the function to calculate the total file size\n",
    "total_file_size = calculate_directory_size(hdfs_host, hdfs_port, directory_path)\n",
    "\n",
    "# Display the total file size\n",
    "print(\"Total File Size:\", total_file_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618a678-c37c-49dc-bea2-55c99807a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--N', type=int, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_N)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_N(self, _, word_counts):\n",
    "        N = self.options.N\n",
    "        top_N_words = heapq.nlargest(N, word_counts)\n",
    "        for count, word in top_N_words:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec4f6-aed0-4200-a6e2-347554571fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python top_n_words.py <input_file> --N <number_of_top_words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f157d-ffea-4697-b07f-6d73bb0d325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "import requests\n",
    "\n",
    "# NameNode URL\n",
    "namenode_url = 'http://<namenode_host>:<namenode_port>'\n",
    "\n",
    "# DataNode URLs\n",
    "datanode_urls = [\n",
    "    'http://<datanode1_host>:<datanode1_port>',\n",
    "    'http://<datanode2_host>:<datanode2_port>',\n",
    "    # Add more DataNode URLs if required\n",
    "]\n",
    "\n",
    "def check_namenode_health():\n",
    "    # Get the NameNode health status\n",
    "    url = f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['beans'][0]['State']\n",
    "        return state\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def check_datanode_health():\n",
    "    # Check the health status of each DataNode\n",
    "    for datanode_url in datanode_urls:\n",
    "        url = f'{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*'\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            state = data['beans'][0]['State']\n",
    "            print(f'DataNode at {datanode_url} is {state}')\n",
    "        else:\n",
    "            print(f'Failed to fetch health status for DataNode at {datanode_url}')\n",
    "\n",
    "# Check NameNode health\n",
    "namenode_state = check_namenode_health()\n",
    "print(f'NameNode is {namenode_state}')\n",
    "\n",
    "# Check DataNode health\n",
    "check_datanode_health()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba9b4d-93ad-4036-96df-3ebe09fb175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "from pywebhdfs.webhdfs import PyWebHdfsClient\n",
    "\n",
    "# HDFS connection details\n",
    "hdfs_host = '<hdfs_host>'\n",
    "hdfs_port = '<hdfs_port>'\n",
    "hdfs_user = '<hdfs_user>'\n",
    "\n",
    "# HDFS path to list\n",
    "hdfs_path = '/path/to/directory'\n",
    "\n",
    "# Create a WebHDFS client\n",
    "client = PyWebHdfsClient(host=hdfs_host, port=hdfs_port, user_name=hdfs_user)\n",
    "\n",
    "# List files and directories in the specified HDFS path\n",
    "response = client.list_dir(hdfs_path)\n",
    "\n",
    "# Process the response\n",
    "if response['boolean']:\n",
    "    # List operation successful\n",
    "    file_status_list = response['FileStatuses']['FileStatus']\n",
    "    for file_status in file_status_list:\n",
    "        file_type = file_status['type']\n",
    "        file_path = file_status['pathSuffix']\n",
    "        print(f\"{file_type}: {file_path}\")\n",
    "else:\n",
    "    # List operation failed\n",
    "    error_message = response['RemoteException']['message']\n",
    "    print(f\"Failed to list files and directories: {error_message}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f15ab-f05a-415e-b557-25a21570b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "import requests\n",
    "\n",
    "# NameNode URL\n",
    "namenode_url = 'http://<namenode_host>:<namenode_port>'\n",
    "\n",
    "def get_datanodes_storage_utilization():\n",
    "    # Get the storage utilization of each DataNode\n",
    "    url = f'{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        live_nodes = data['beans'][0]['LiveNodes']\n",
    "        datanodes = live_nodes.split(';')\n",
    "        \n",
    "        # Dictionary to store DataNode storage utilization\n",
    "        datanode_utilization = {}\n",
    "\n",
    "        # Iterate over each DataNode\n",
    "        for datanode in datanodes:\n",
    "            datanode_info = datanode.split(',')\n",
    "            datanode_name = datanode_info[0]\n",
    "            used_storage = int(datanode_info[1])\n",
    "            capacity_storage = int(datanode_info[2])\n",
    "            utilization_percentage = round((used_storage / capacity_storage) * 100, 2)\n",
    "            datanode_utilization[datanode_name] = utilization_percentage\n",
    "        \n",
    "        return datanode_utilization\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def get_highest_and_lowest_utilization_nodes(datanode_utilization):\n",
    "    # Find the nodes with the highest and lowest storage utilization\n",
    "    highest_utilization_node = max(datanode_utilization, key=datanode_utilization.get)\n",
    "    lowest_utilization_node = min(datanode_utilization, key=datanode_utilization.get)\n",
    "    highest_utilization = datanode_utilization[highest_utilization_node]\n",
    "    lowest_utilization = datanode_utilization[lowest_utilization_node]\n",
    "    \n",
    "    return highest_utilization_node, highest_utilization, lowest_utilization_node, lowest_utilization\n",
    "\n",
    "# Get DataNode storage utilization\n",
    "datanode_utilization = get_datanodes_storage_utilization()\n",
    "\n",
    "if datanode_utilization:\n",
    "    # Print the DataNode storage utilization\n",
    "    for datanode, utilization in datanode_utilization.items():\n",
    "        print(f\"DataNode: {datanode}, Utilization: {utilization}%\")\n",
    "    \n",
    "    # Get the nodes with the highest and lowest storage utilization\n",
    "    highest_node, highest_utilization, lowest_node, lowest_utilization = get_highest_and_lowest_utilization_nodes(datanode_utilization)\n",
    "    print(f\"\\nNode with highest storage utilization: {highest_node}, Utilization: {highest_utilization}%\")\n",
    "    print(f\"Node with lowest storage utilization: {lowest_node}, Utilization: {lowest_utilization}%\")\n",
    "else:\n",
    "    print(\"Failed to fetch DataNode storage utilization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1da97-d41d-4785-899e-9f550dccfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "import requests\n",
    "\n",
    "# ResourceManager URL\n",
    "resourcemanager_url = 'http://<resourcemanager_host>:<resourcemanager_port>'\n",
    "\n",
    "# Submit a Hadoop job\n",
    "def submit_hadoop_job(jar_path, class_name, arguments):\n",
    "    url = f'{resourcemanager_url}/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        application_id = data['application-id']\n",
    "        submit_url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/app'\n",
    "        payload = {\n",
    "            'application-id': application_id,\n",
    "            'application-name': 'Hadoop Job',\n",
    "            'am-container-spec': {\n",
    "                'local-resources': {\n",
    "                    'entry': [\n",
    "                        {'key': 'app.jar', 'value': {'resource': jar_path, 'type': 'FILE'}},\n",
    "                    ]\n",
    "                },\n",
    "                'commands': {\n",
    "                    'command': f'java -cp app.jar {class_name} {\" \".join(arguments)}'\n",
    "                },\n",
    "                'environment': {\n",
    "                    'entry': [\n",
    "                        {'key': 'CLASSPATH', 'value': 'app.jar'}\n",
    "                    ]\n",
    "                },\n",
    "                'resource': {'memory': 1024, 'vCores': 1}\n",
    "            },\n",
    "            'unmanaged-AM': False,\n",
    "            'max-app-attempts': 1,\n",
    "            'resource': {'memory': 1024, 'vCores': 1},\n",
    "            'application-type': 'MAPREDUCE'\n",
    "        }\n",
    "        response = requests.post(submit_url, json=payload)\n",
    "        if response.status_code == 202:\n",
    "            print('Hadoop job submitted successfully.')\n",
    "            return application_id\n",
    "        else:\n",
    "            print('Failed to submit Hadoop job.')\n",
    "    else:\n",
    "        print('Failed to get new application ID.')\n",
    "\n",
    "# Get Hadoop job status\n",
    "def get_hadoop_job_status(application_id):\n",
    "    url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        state = data['app']['state']\n",
    "        progress = data['app']['progress']\n",
    "        final_status = data['app']['finalStatus']\n",
    "        return state, progress, final_status\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "# Get Hadoop job final output\n",
    "def get_hadoop_job_output(application_id):\n",
    "    url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        attempt_id = data['appAttempts']['appAttempt'][0]['appAttemptId']\n",
    "        containers_url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts/{attempt_id}/containers'\n",
    "        response = requests.get(containers_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            container_id = data['containers']['container'][0]['id']\n",
    "            logs_url = f'{resourcemanager_url}/proxy/{container_id}/ws/v1/mapreduce/jobs'\n",
    "            response = requests.get(logs_url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                logs = data['jobs']['job'][0]['diagnostics']\n",
    "                return logs\n",
    "            else:\n",
    "                return 'Failed to retrieve job logs.'\n",
    "        else:\n",
    "            return 'Failed to retrieve containers information.'\n",
    "    else:\n",
    "        return 'Failed to retrieve application attempts.'\n",
    "\n",
    "# Submit a Hadoop job\n",
    "application_id = submit_hadoop_job('/path/to/hadoop-job.jar', 'com.example.HadoopJob', ['input', 'output'])\n",
    "\n",
    "if application_id:\n",
    "    # Monitor Hadoop job progress\n",
    "    while True:\n",
    "        state, progress, final_status = get_hadoop_job_status(application_id)\n",
    "        if state == 'RUNNING':\n",
    "            print(f'Job progress: {progress}')\n",
    "        elif state in ('SUCCEEDED', 'FAILED', 'KILLED'):\n",
    "            print(f'Job state: {state}')\n",
    "            print(f'Job final status: {final_status}')\n",
    "            break\n",
    "\n",
    "    # Retrieve Hadoop job output\n",
    "    if state == 'SUCCEEDED':\n",
    "        output = get_hadoop_job_output(application_id)\n",
    "        print(f'Job output: {output}')\n",
    "    else:\n",
    "        print('Job did not succeed.')\n",
    "else:\n",
    "    print('Failed to submit Hadoop job.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde6287-13a9-4842-9d28-869350c91560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "import requests\n",
    "\n",
    "# ResourceManager URL\n",
    "resourcemanager_url = 'http://<resourcemanager_host>:<resourcemanager_port>'\n",
    "\n",
    "# Submit a Hadoop job with resource requirements\n",
    "def submit_hadoop_job(jar_path, class_name, arguments, memory, vcores):\n",
    "    url = f'{resourcemanager_url}/ws/v1/cluster/apps/new-application'\n",
    "    response = requests.post(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        application_id = data['application-id']\n",
    "        submit_url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/app'\n",
    "        payload = {\n",
    "            'application-id': application_id,\n",
    "            'application-name': 'Hadoop Job',\n",
    "            'am-container-spec': {\n",
    "                'local-resources': {\n",
    "                    'entry': [\n",
    "                        {'key': 'app.jar', 'value': {'resource': jar_path, 'type': 'FILE'}},\n",
    "                    ]\n",
    "                },\n",
    "                'commands': {\n",
    "                    'command': f'java -Xmx{memory}m -Xms{memory}m -cp app.jar {class_name} {\" \".join(arguments)}'\n",
    "                },\n",
    "                'environment': {\n",
    "                    'entry': [\n",
    "                        {'key': 'CLASSPATH', 'value': 'app.jar'}\n",
    "                    ]\n",
    "                },\n",
    "                'resource': {'memory': memory, 'vCores': vcores}\n",
    "            },\n",
    "            'unmanaged-AM': False,\n",
    "            'max-app-attempts': 1,\n",
    "            'resource': {'memory': memory, 'vCores': vcores},\n",
    "            'application-type': 'MAPREDUCE'\n",
    "        }\n",
    "        response = requests.post(submit_url, json=payload)\n",
    "        if response.status_code == 202:\n",
    "            print('Hadoop job submitted successfully.')\n",
    "            return application_id\n",
    "        else:\n",
    "            print('Failed to submit Hadoop job.')\n",
    "    else:\n",
    "        print('Failed to get new application ID.')\n",
    "\n",
    "# Get resource usage for a Hadoop job\n",
    "def get_resource_usage(application_id):\n",
    "    url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        attempt_id = data['appAttempts']['appAttempt'][0]['appAttemptId']\n",
    "        containers_url = f'{resourcemanager_url}/ws/v1/cluster/apps/{application_id}/appattempts/{attempt_id}/containers'\n",
    "        response = requests.get(containers_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            containers = data['containers']['container']\n",
    "            resource_usage = []\n",
    "            for container in containers:\n",
    "                container_id = container['id']\n",
    "                container_memory = container['allocatedMB']\n",
    "                container_vcores = container['allocatedVCores']\n",
    "                resource_usage.append((container_id, container_memory, container_vcores))\n",
    "            return resource_usage\n",
    "        else:\n",
    "            return 'Failed to retrieve containers information.'\n",
    "    else:\n",
    "        return 'Failed to retrieve application attempts.'\n",
    "\n",
    "# Submit a Hadoop job with resource requirements\n",
    "application_id = submit_hadoop_job('/path/to/hadoop-job.jar', 'com.example.HadoopJob', ['input', 'output'], 1024, 1)\n",
    "\n",
    "if application_id:\n",
    "    # Monitor resource usage for the Hadoop job\n",
    "    while True:\n",
    "        resource_usage = get_resource_usage(application_id)\n",
    "        if isinstance(resource_usage, list):\n",
    "            for container_id, container_memory, container_vcores in resource_usage:\n",
    "                print(f'Container ID: {container_id}, Memory: {container_memory}MB, vCores: {container_vcores}')\n",
    "        else:\n",
    "            print(resource_usage)\n",
    "        # Wait for a specific interval before checking resource usage again\n",
    "        # You can adjust the sleep interval based on your requirements\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print('Failed to submit Hadoop job.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ce673-4702-41b5-9e50-de234411f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Input file path\n",
    "input_file = '/path/to/input/file.txt'\n",
    "\n",
    "# MapReduce job command\n",
    "job_command = f'hadoop jar /path/to/hadoop-streaming.jar -input {input_file} -output output -mapper mapper.py -reducer reducer.py'\n",
    "\n",
    "# List of input split sizes to compare\n",
    "split_sizes = [10, 100, 1000]\n",
    "\n",
    "# Function to run MapReduce job and measure execution time\n",
    "def run_mapreduce_job(split_size):\n",
    "    start_time = time.time()\n",
    "    # Set input split size\n",
    "    subprocess.run(f'hdfs dfs -D mapreduce.input.fileinputformat.split.maxsize={split_size} -rm -r -f output', shell=True)\n",
    "    # Run MapReduce job\n",
    "    subprocess.run(job_command, shell=True)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return execution_time\n",
    "\n",
    "# Compare MapReduce job performance for different input split sizes\n",
    "for split_size in split_sizes:\n",
    "    execution_time = run_mapreduce_job(split_size)\n",
    "    print(f'Input Split Size: {split_size} - Execution Time: {execution_time} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
